{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2925605c-847a-4131-a1f2-244d34811261",
   "metadata": {},
   "source": [
    "Machine Leaning models ðŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277155f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Machine Leaning models\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Scope of this class: </b>\n",
    "In this session we will use, study and implement classical machine learning models on simple data. \n",
    "</div>\n",
    "\n",
    "The last exercises of the session will be devoted to the implementation of more complex models and the use of a two in order to solve a task on a real dataset.\n",
    "\n",
    "\n",
    "\n",
    "The objectives of this lab: **Discover, use and implement from scratch many classical machine learning algorithms !**\n",
    "\n",
    "Note that in machine learning and in informatic in general, when you do not know how to do something or how does something work, your first reflex should be to look on **the documentation** and more generally on internet. First, all the documentation of the libraries you will use is there. Moreover, Python has a huge community (one of its strength) and therefore a lot of trouble you may have has already been resolved on a forume (such as StackOverflow). \n",
    "\n",
    "**However**, you should know that code you found on forum might be under some copyrights it is the case for instance on all the code avaible on StackOverflow\n",
    "\n",
    "To find the documentation on a class, a function or a method from a library, most of the time, searching the name of the library and the name of the method will give you the documentation as the first link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de973-2f86-4606-b766-04a83968e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, Tuple, List #Typing library : In python the types of variables\n",
    "# are not declared explicitly.\n",
    "#However, it is sometimes useful to specify them to improve the readability of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af259d-6f9a-4b37-abaa-dc7b8dbedaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eaed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set() # graphical library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fcdd8-5f20-4d5d-bfd6-02147ed151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn is the reference library in python for using machine learning models (mainly on tabular data)\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16972a7-57d3-4155-8f10-911a63437c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# can use named colors or HTML codes\n",
    "colormap = np.array(['red', 'lightseagreen', '#F39C12'])\n",
    "cmap = sns.dark_palette((200, 1000,45), input=\"husl\",as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c4f5f",
   "metadata": {},
   "source": [
    "## 1.0 Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2624a",
   "metadata": {},
   "source": [
    "This first exercise has two purposes: \n",
    "\n",
    "- To introduce you to the notion of class with a data science-oriented practical case\n",
    "\n",
    "- To see under the hood the notion of data scaling\n",
    "\n",
    "\n",
    "Many normalization methods are possible, but in 99% of the use cases.\n",
    "you will use the `StandardScaler` : $$Z= \\frac{X - \\bar{X}}{\\sigma(X)}$$ or `MinMaxScaler`: $$S = \\frac{X - \\min(X)}{\\max(X) - \\min(X)}$$.\n",
    "The first set your data between 0 and 1 and the second to mean 0 and standard deviation 1. \n",
    "\n",
    "Why should you scale your data?\n",
    "\n",
    "Scale effects can interfere with the training of many estimators. It is necessary to scale the data where the estimator uses distance or gradient during training.\n",
    "\n",
    "Advice: always normalize your data except if you want to use an algorithm based on decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b31c9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.0.1 : </b>\n",
    "\n",
    "Implement the two normalization methods using the object formalism.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdf1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.epsilon = 1e-6\n",
    "    \n",
    "    def fit(self, X: np.array, y=None): # y=None is a scikit learn convention\n",
    "        # Compute the mean for each columns\n",
    "        self.mean = ...\n",
    "        # Compute the standard deviation for each columns\n",
    "        self.std = ...\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_norm = ... # safe division\n",
    "        return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalizer:\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.epsilon = 1e-6\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # the floor is yours\n",
    "        ...\n",
    "    \n",
    "    def transform(self, X:np.ndarray) -> np.ndarray:\n",
    "        # (X - X_min) / (X_max - X_min) \n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474620fd",
   "metadata": {},
   "source": [
    "## 1.1 Generate the toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e800944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "def eval_model(Y_pred:np.ndarray, Y_true:np.ndarray, classification:bool=True):\n",
    "    metric = accuracy_score if classification else mean_squared_error\n",
    "    score = metric(Y_true, Y_pred)\n",
    "    return f\"The score is {score:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b169d-4a0e-4dc4-a9b5-129eb0f5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_dataset(mode: str = \"train\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    random_state = 42 if mode == \"train\" else 1337\n",
    "    X, y = make_moons(n_samples=1000,random_state=random_state, noise=0.20)\n",
    "    return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab11a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_dataset(mode: str = \"train\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    random_state = 42 if mode == \"train\" else 1337\n",
    "    np.random.seed(random_state)\n",
    "    X_1 = np.random.uniform(size = 1000, low = 0, high = 25)\n",
    "    X_2 = np.random.uniform(size = 1000, low = 0, high = 8)\n",
    "    gaussian_noise = np.random.normal(loc=0, scale=4, size=1000)\n",
    "    Y = 2 * X_1 - 3 * X_2 + gaussian_noise\n",
    "    X = np.stack([X_1, X_2], axis =-1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82eb5cc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.1 : </b>\n",
    "\n",
    "Using the previous functions : Generate a train and test dataset for the two types of task (classification and regression)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb7597-177e-4281-9fb3-19f82d6253a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "X_train_clf, Y_train_clf = generate_classification_dataset(mode=\"train\")\n",
    "X_test_clf, Y_test_clf = generate_classification_dataset(mode=\"test\")\n",
    "\n",
    "X_train_reg, Y_train_reg = generate_regression_dataset(mode=\"train\")\n",
    "X_test_reg, Y_test_reg = generate_regression_dataset(mode=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfb7ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.2 : </b>\n",
    "\n",
    "Use your favorite normalization to scale your data .\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "#X_train_clf_norm = ...\n",
    "#X_test_clf_norm = ...\n",
    "\n",
    "#X_train_reg_norm = ...\n",
    "#X_test_reg_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcf903-65e6-4224-9d05-f95493fbe6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X:np.ndarray, Y: np.ndarray, colormap: np.ndarray = colormap,classification=True) -> None:\n",
    "    if classification:\n",
    "        plt.scatter(X[:, 0], X[:, 1], color=colormap[Y],s=10)\n",
    "    else:\n",
    "        \n",
    "        points = plt.scatter(X[:, 0], X[:, 1], c=Y, s=15, cmap=cmap)\n",
    "        plt.colorbar(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d250d65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.3 : </b>\n",
    "\n",
    "Briefly explain this dataset (features and the machine learning task to be performed).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dcf51-a879-4d25-8d88-410c1e537862",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X=X_train_clf, Y=Y_train_clf, colormap=colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785f9b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.1.4 : </b>\n",
    "\n",
    "Briefly explain the dataset (features and the machine learning task to be performed).\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X=X_train_reg, Y=Y_train_reg, colormap=colormap,classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3451924",
   "metadata": {},
   "source": [
    "## 1.2 General information on machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736ea5d",
   "metadata": {},
   "source": [
    "It is possible to implement a machine learning model in many ways. However, there are **code standards** that allow for readability and understanding by all. It is common to implement a machine learning model with the sklearn framework. We will ask you to implement your methods in the form of a **python class** with at least two methods: the **fit** method and the **predict** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5870f0-65bd-4f6f-9f0a-bfe4177d1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningModel(Protocol): \n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, *args, **kwargs):\n",
    "        ...\n",
    "    def predict(X: np.ndarray, *args, **kwargs) -> np.ndarray:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68452a3a-1943-48e0-a73c-896a62689335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X: np.ndarray, Y: np.ndarray, classifier: MachineLearningModel):\n",
    "    # Plotting decision regions\n",
    "    f = plt.figure()\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plot_dataset(X=X, Y=Y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6647e2b",
   "metadata": {},
   "source": [
    "A machine learning model is a function  $f_\\theta$ parametrized by a set of parameters $\\theta$.\n",
    "    \n",
    "* Train a model means : Find the best $\\theta$ parameters $\\theta^*$ that minimize a given loss function $\\mathcal{L}$.\n",
    "\n",
    "\n",
    "* The loss function $\\mathcal{L}$ allows to evaluate the prediction error of the model with respect to a ground truth or label $y$ :  $$err(y,\\hat{y}) = \\mathcal{L}(y,f_\\theta(x))$$\n",
    "\n",
    "\n",
    "* These theta parameters can be found iteratively by following the opposite direction of the gradient of the loss function.\n",
    "\\begin{equation}\n",
    "\\theta_{i+1}=\\theta_{i}-\\tau_{i} \\nabla \\mathcal{L}\\left(\\theta_{i}\\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a3fa7",
   "metadata": {},
   "source": [
    "<img src=\"./images/ball.png\" alt=\"tree\" width=\"600\"/>\n",
    "source:rasbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b673ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b> Example : </b>\n",
    "Let's imagine that we want to train a model to predict the life expectancy $y$ according to some biological data $x$ : age, weight, height, blood profile ...\n",
    "We have to find by training the set of parameters $\\theta$ of our model $f_\\theta$ which minimizes the prediction error. This error is defined as the distance between the predicted life expectancy $\\hat{y}$ and the true life expectancy $y$ : \n",
    "    \n",
    "$$err(y,\\hat{y}) = \\mathcal{L}(y,f_\\theta(x))$$ with : $$\\mathcal{L}(y,\\hat{y}) = \\Vert y - \\hat{y} \\Vert_2$$ (MSE loss)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5ac9d",
   "metadata": {},
   "source": [
    "## 1.3 Machine learning models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7dae38",
   "metadata": {},
   "source": [
    "### 1.3.1 A first classification model: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b1dcf",
   "metadata": {},
   "source": [
    "The logistic regression model is the basic model in binary classification. It has the advantage of being very simple and easily explained. Despite its simplicity, it is widely used, especially in the banking world.\n",
    "\\begin{equation}\n",
    "S\\left(X^{(i)}\\right)=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\theta_{3} x_{3}+\\ldots .+\\theta_{n} x_{n} = \\sum_{i=0}^{n+1}\\left(\\theta_{i} x_{i}\\right) = \\Theta X^{(i)}\n",
    "\\end{equation}\n",
    "with $\\theta_i \\in \\mathbf{R}$ for all $i \\in 1,2,\\ldots,n$.\n",
    "\n",
    "$\\Theta$ is the vector of parameters to be estimated and $S$ is called the score function.\n",
    "The idea of the logistic regression is to find coefficients $\\theta_1,\\theta_2,\\ldots,\\theta_n$ such that \n",
    "* $S\\left(X^{(i)}\\right)>0$ when the label of $i$ is $1$.\n",
    "* $S\\left(X^{(i)}\\right)<0$ when the label of $i$ is $0$.\n",
    "\n",
    "However, a classification model should return a probability of belonging to a class and not a score.\n",
    "To go from a score function to a probability we will use the logistic function (or sigmoid) : \n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{Sigmoid}(x)= \\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "The probability that individual $i$ belongs to class $1$ is therefore modeled by :\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=1 \\mid X^{(i)} , \\Theta) = \\sigma(S\\left(X^{(i)}\\right))\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P(y=0 \\mid X^{(i)} , \\Theta) = 1-P(y=1 \\mid X^{(i)} , \\Theta) = 1 - \\sigma(S\\left(X^{(i)}\\right))\n",
    "\\end{equation}\n",
    "\n",
    "In practice if $X$ is the sample matrix, $\\Theta$ the parameters vector and $\\sigma$ the sigmoid function.\n",
    "So, the logistic model is defined as :\n",
    "\n",
    "<span style=\"color:red\"> $$M(X,\\Theta) = \\sigma(X \\Theta) $$</span>\n",
    "\n",
    "The best $\\theta$ parameters can be found iteratively by following the opposite direction of the gradient of the loss function defined as follow :\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{CE}}(\\hat{y}, y)=-\\log p(y \\mid x)=-[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})]\n",
    "\\end{equation}\n",
    "this loss is called the cross-entropy.\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{CE}}(\\hat{y}, y)=-[y \\log \\sigma(\\theta \\cdot x)+(1-y) \\log (1-\\sigma(\\theta \\cdot x))]\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\theta}=[y-\\sigma(\\theta \\cdot x)] x=[\\hat{y}-y] x\n",
    "\\end{equation}</span>\n",
    "\n",
    "**The gradients are noted in a vector manner to facilitate the transition to the implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261971c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b> Note : </b>\n",
    "    In machine learning, understanding the dimension of tensors is essential. This allows to better understand the algorithms. Knowing the dimensions in Machine Learning is a bit like dimensional analysis in physics. If I know the shape of my tensors and the operations I perform then I can understand the output shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849a684",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.1 : </b>\n",
    "\n",
    "What is the dimension $X$, $\\Theta$ and the output $\\hat{y}$ of the model ?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b09f55",
   "metadata": {},
   "source": [
    "#### Sklearn logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c9a7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.2 : </b>\n",
    "\n",
    "1. Train a sklearn logistic regression model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score and the most beautiful decision boundaries.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fb0ca",
   "metadata": {},
   "source": [
    "#### Homemade logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe33806",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.3 : </b>\n",
    "\n",
    "Implement logistic regression from scratch.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice\n",
    "class Sigmoid:\n",
    "    def __call__(self,x):\n",
    "        return ...\n",
    "class HomemadeLogisticRegression(AibtMachineLearningModel):\n",
    "    \"\"\" Logistic Regression classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    gradient_descent: boolean\n",
    "        True or false depending if gradient descent should be used when training. If\n",
    "        false then we use batch optimization by least squares.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        self.param = None\n",
    "        self.learning_rate = ...\n",
    "        self.sigmoid = ...\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.param = np.random.uniform(..., ..., (...,))\n",
    "\n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        self._initialize_parameters(X) # Initially the theta parameters are chosen randomly.\n",
    "        # Tune parameters for n iterations\n",
    "        for i in range(n_iterations):\n",
    "            # Make a new prediction\n",
    "            y_pred = ... # Apply the red equation of the logistic model\n",
    "            # Move against the gradient of the loss function with\n",
    "            # respect to the parameters to minimize the loss\n",
    "            self.param -= ... # Tips : Have a look to the blue equation\n",
    "    def predict(self, X):\n",
    "        y_pred = ... # Predict the class of x_i\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca517c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/logistic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8711d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.4 : </b>\n",
    "\n",
    "1. Train your homemade logistic regression model on the train data. \n",
    "\n",
    "2. Display the decision boundaries.\n",
    "\n",
    "3. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3851937-69bd-4795-9976-1f3c8c77ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bef269",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.5 : </b>\n",
    "\n",
    "In your mind, why are the decision boundaries and score between your model and the sklearn model different ?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3701f64-a868-441e-95b2-7a26936ade6f",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315eb90-b868-4d52-acdd-22e858c946f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.6 : </b>\n",
    "\n",
    "Why is it important to normalize your data for logistic regression?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513daea-7de5-4d4c-9189-93820172ddc9",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139f2e6-e9f3-4298-9f68-8feb8540ad05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.1.7 : </b>\n",
    "\n",
    "How can you interpret the model coefficient's ?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746b8ff-8bc8-429c-8181-371497a6e9e2",
   "metadata": {},
   "source": [
    "Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81eac4",
   "metadata": {},
   "source": [
    "### 1.3.2 A first regression model: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d7e71",
   "metadata": {},
   "source": [
    "Previously you have seen the logistic regression. Linear regression is its equivalent for classification problems.\n",
    "If you understand logistic regression then this part will be super simple!\n",
    "The simple linear regression model is the basic model simple regression. It has the advantage of being very simple and easily explained. Despite its simplicity, it is widely used, especially in physics, statistics and biology.\n",
    "The prediction for a sample $X^{(i)}$ is defined as :\n",
    "\\begin{equation}\n",
    "S\\left(X^{(i)}\\right)=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\theta_{3} x_{3}+\\ldots .+\\theta_{n} x_{n} + \\beta = \\sum_{i=0}^{n+1}\\left(\\theta_{i} x_{i}\\right) + \\beta = \\Theta X^{(i)} + \\beta\n",
    "\\end{equation}\n",
    "with $\\theta_i \\in \\mathbf{R}$ for all $i \\in 1,2,\\ldots,n$ and $\\beta \\in \\mathbf{R}$ .\n",
    "As you can see we have introduced a new beta variable. It corresponds to the **bias of the model**. For pedagogical reasons we did not introduce it for the logistic model (we considered the case where $\\beta=0$)\n",
    "\n",
    "\n",
    "\n",
    "In practice if $X$ is the sample matrix, $\\Theta$ the parameters vector and $\\sigma$ the sigmoid function.\n",
    "So, the logistic model is defined as :\n",
    "\n",
    "<span style=\"color:red\"> $$M(X,\\Theta,\\beta) = X \\Theta + \\beta\\mathbf{1} $$</span>\n",
    "where $\\mathbf{1} \\in \\mathbf{R}^N $ is the $N$-dimensional vector where all coordinates are equal to $1$, (with $N$ equal to the number of samples in $X$)\n",
    "\n",
    "We want to find the best $\\theta$ and $\\beta$ such that : \n",
    "    $$L_{MSE}(\\hat{y}-y)= \\|\\hat{Y}-Y\\|^{2} = \\|(X \\Theta+\\beta \\mathbf{1})-Y\\|^{2}$$\n",
    "\n",
    "Thanks to the gradient algorithm, we can iteratively find the best parameters.\n",
    "\n",
    "The partial derivative of the parameters $\\theta$ is given by : \n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\theta}=[y-\\sigma(\\theta \\cdot x)] x=[\\hat{y}-y] x\n",
    "\\end{equation}</span>\n",
    "\n",
    "The partial derivative of the parameters $\\beta$ is given by : \n",
    "<span style=\"color:blue\">\\begin{equation}\n",
    "\\frac{\\partial L_{\\mathrm{CE}}(\\hat{y}, y)}{\\partial \\beta}=[y-\\sigma(\\theta \\cdot x)] x=[\\hat{y}-y]\n",
    "\\end{equation}</span>\n",
    "<img src=\"./images/gradient_descent_parameter_a.gif\" alt=\"tree\" width=\"600\"/>\n",
    "source:baptiste-monpezat\n",
    "\n",
    "\n",
    "**The gradients are noted in a vector manner to facilitate the transition to the implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005e814",
   "metadata": {},
   "source": [
    "#### Sklearn linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47293ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.1 : </b>\n",
    "\n",
    "1. Train a sklearn linear regression model on the train data. \n",
    "\n",
    "2. Evaluate the model on the test data using the `eval_model` method defined above.\n",
    "\n",
    "4. Try to get the best score.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5940a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1543c2-fc30-4fb2-86ad-4eb57c3f081e",
   "metadata": {},
   "source": [
    "#### Homemade linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54692bea-bd41-49f7-9e25-a741315cd410",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b> Exercise 1.3.2.2 : </b>\n",
    "\n",
    "1. Train your homemade logistic regression model on the train data. \n",
    "2. Evaluate the model on the test data `eval_model` method defined above.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85758edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomemadeSimpleLinearRegression(AibtMachineLearningModel):\n",
    "    \"\"\" Logistic Regression classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    gradient_descent: boolean\n",
    "        True or false depending if gradient descent should be used when training. If\n",
    "        false then we use batch optimization by least squares.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.param = None\n",
    "        self.bias = None\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.param = ...\n",
    "        self.bias = ...\n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        self._initialize_parameters(X) # Initially the theta parameters are chosen randomly.\n",
    "        # Tune parameters for n iterations\n",
    "        for i in range(n_iterations):\n",
    "            # Make a new prediction\n",
    "            y_pred = ...\n",
    "            # Move against the gradient of the loss function with\n",
    "            # respect to the parameters to minimize the loss\n",
    "            self.param -= ...\n",
    "            self.bias -= ...\n",
    "    def predict(self, X):\n",
    "        y_pred = ...\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
